{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "import json\n",
    "import pandas as pd\n",
    "# pt.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index Variant</th>\n",
       "      <th>Ranking Model</th>\n",
       "      <th>NDCG</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <th>NDCG@20</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Precision@20</th>\n",
       "      <th>Recall@5</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>Recall@20</th>\n",
       "      <th>MRR</th>\n",
       "      <th>Mean Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>full_index</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>full_index</td>\n",
       "      <td>LM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stopwords_removed</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stopwords_removed</td>\n",
       "      <td>LM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stemming</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stemming</td>\n",
       "      <td>LM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stopwords_removed_stemming</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stopwords_removed_stemming</td>\n",
       "      <td>LM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Index Variant Ranking Model  NDCG  NDCG@5  NDCG@10  NDCG@20   \n",
       "0                  full_index          BM25   0.0     0.0      0.0      0.0  \\\n",
       "1                  full_index            LM   0.0     0.0      0.0      0.0   \n",
       "2           stopwords_removed          BM25   0.0     0.0      0.0      0.0   \n",
       "3           stopwords_removed            LM   0.0     0.0      0.0      0.0   \n",
       "4                    stemming          BM25   0.0     0.0      0.0      0.0   \n",
       "5                    stemming            LM   0.0     0.0      0.0      0.0   \n",
       "6  stopwords_removed_stemming          BM25   0.0     0.0      0.0      0.0   \n",
       "7  stopwords_removed_stemming            LM   0.0     0.0      0.0      0.0   \n",
       "\n",
       "   Precision@5  Precision@10  Precision@20  Recall@5  Recall@10  Recall@20   \n",
       "0          0.0           0.0           0.0       0.0        0.0        0.0  \\\n",
       "1          0.0           0.0           0.0       0.0        0.0        0.0   \n",
       "2          0.0           0.0           0.0       0.0        0.0        0.0   \n",
       "3          0.0           0.0           0.0       0.0        0.0        0.0   \n",
       "4          0.0           0.0           0.0       0.0        0.0        0.0   \n",
       "5          0.0           0.0           0.0       0.0        0.0        0.0   \n",
       "6          0.0           0.0           0.0       0.0        0.0        0.0   \n",
       "7          0.0           0.0           0.0       0.0        0.0        0.0   \n",
       "\n",
       "   MRR  Mean Time  \n",
       "0  0.0        0.0  \n",
       "1  0.0        0.0  \n",
       "2  0.0        0.0  \n",
       "3  0.0        0.0  \n",
       "4  0.0        0.0  \n",
       "5  0.0        0.0  \n",
       "6  0.0        0.0  \n",
       "7  0.0        0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('output/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Please confirm you agree to the MSMARCO data usage agreement found at <http://www.msmarco.org/dataset.aspx>\n",
      "[INFO] If you have a local copy of https://msmarco.blob.core.windows.net/msmarcoranking/collectionandqueries.tar.gz, you can symlink it here to avoid downloading it again: /home/nicklas/.ir_datasets/downloads/31644046b18952c1386cd4564ba2ae69\n",
      "[INFO] [starting] https://msmarco.blob.core.windows.net/msmarcoranking/collectionandqueries.tar.gz\n",
      "                                                                                \n",
      "\u001b[A                                                                                                                        [INFO] [error] https://msmarco.blob.core.windows.net/msmarcoranking/collectionandqueries.tar.gz: [00:26] [28.2MB] [1.08MB/s]\n",
      "msmarco-passage/dev/small documents:   0%|          | 0/8841823 [00:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m# index_ref = pt.IndexRef.of('./indices/msmarco-passage') # assumes you have already built an index\u001b[39;00m\n\u001b[1;32m      3\u001b[0m indexer \u001b[39m=\u001b[39m pt\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mIterDictIndexer(\u001b[39m'\u001b[39m\u001b[39m./msmarco-passage\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m indexref \u001b[39m=\u001b[39m indexer\u001b[39m.\u001b[39;49mindex(dataset\u001b[39m.\u001b[39;49mget_corpus_iter())\n\u001b[1;32m      5\u001b[0m index \u001b[39m=\u001b[39m pt\u001b[39m.\u001b[39mIndexFactory\u001b[39m.\u001b[39mof(indexref)\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/pyterrier/index.py:968\u001b[0m, in \u001b[0;36m_IterDictIndexer_fifo.index\u001b[0;34m(self, it, fields, meta, meta_lengths)\u001b[0m\n\u001b[1;32m    965\u001b[0m     fifos\u001b[39m.\u001b[39mappend(fifo)\n\u001b[1;32m    967\u001b[0m \u001b[39m# Start dishing out the docs to the fifos\u001b[39;00m\n\u001b[0;32m--> 968\u001b[0m threading\u001b[39m.\u001b[39mThread(target\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_write_fifos, args\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_filter_iterable(it, fields), fifos), daemon\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mstart()\n\u001b[1;32m    970\u001b[0m \u001b[39m# Different process for memory indexer (still taking advantage of faster fifos)\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[39mif\u001b[39;00m Indexer \u001b[39mis\u001b[39;00m BasicMemoryIndexer:\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/pyterrier/index.py:824\u001b[0m, in \u001b[0;36m_BaseIterDictIndexer._filter_iterable\u001b[0;34m(self, it, indexed_fields)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    822\u001b[0m     all_fields \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdocno\u001b[39m\u001b[39m'\u001b[39m} \u001b[39m|\u001b[39m \u001b[39mset\u001b[39m(indexed_fields) \u001b[39m|\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m--> 824\u001b[0m (first_doc,), it \u001b[39m=\u001b[39m more_itertools\u001b[39m.\u001b[39;49mspy(it) \u001b[39m# peek at the first document and validate it\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_doc_dict(first_doc)\n\u001b[1;32m    827\u001b[0m \u001b[39m# important: return an iterator here, rather than make this function a generator,\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[39m# to be sure that the validation above happens when _filter_iterable is called,\u001b[39;00m\n\u001b[1;32m    829\u001b[0m \u001b[39m# rather than on the first invocation of next()\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/more_itertools/more.py:1045\u001b[0m, in \u001b[0;36mspy\u001b[0;34m(iterable, n)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a 2-tuple with a list containing the first *n* elements of\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[39m*iterable*, and an iterator with the same items as *iterable*.\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[39mThis allows you to \"look ahead\" at the items in the iterable without\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \n\u001b[1;32m   1043\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m it \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(iterable)\n\u001b[0;32m-> 1045\u001b[0m head \u001b[39m=\u001b[39m take(n, it)\n\u001b[1;32m   1047\u001b[0m \u001b[39mreturn\u001b[39;00m head\u001b[39m.\u001b[39mcopy(), chain(head, it)\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/more_itertools/recipes.py:93\u001b[0m, in \u001b[0;36mtake\u001b[0;34m(n, iterable)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtake\u001b[39m(n, iterable):\n\u001b[1;32m     81\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return first *n* items of the iterable as a list.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m \u001b[39m        >>> take(3, range(10))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \n\u001b[1;32m     92\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(islice(iterable, n))\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/pyterrier/datasets.py:418\u001b[0m, in \u001b[0;36mIRDSDataset.get_corpus_iter.<locals>.gen\u001b[0;34m()\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgen\u001b[39m():\n\u001b[0;32m--> 418\u001b[0m     \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    419\u001b[0m         doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39m_asdict()\n\u001b[1;32m    420\u001b[0m         \u001b[39m# pyterrier uses \"docno\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/__init__.py:147\u001b[0m, in \u001b[0;36mDocstoreSplitter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mit)\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/formats/tsv.py:92\u001b[0m, in \u001b[0;36mTsvIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 92\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mline_iter)\n\u001b[1;32m     93\u001b[0m     cols \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mrstrip(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m     num_cols \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls\u001b[39m.\u001b[39m_fields)\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/formats/tsv.py:28\u001b[0m, in \u001b[0;36mFileLineIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctxt\u001b[39m.\u001b[39menter_context(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdlc[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_idx]\u001b[39m.\u001b[39mstream()))\n\u001b[1;32m     27\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctxt\u001b[39m.\u001b[39;49menter_context(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdlc\u001b[39m.\u001b[39;49mstream()))\n\u001b[1;32m     29\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart:\n\u001b[1;32m     30\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mreadline()\n",
      "File \u001b[0;32m/usr/lib/python3.8/contextlib.py:425\u001b[0m, in \u001b[0;36m_BaseExitStack.enter_context\u001b[0;34m(self, cm)\u001b[0m\n\u001b[1;32m    423\u001b[0m _cm_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(cm)\n\u001b[1;32m    424\u001b[0m _exit \u001b[39m=\u001b[39m _cm_type\u001b[39m.\u001b[39m\u001b[39m__exit__\u001b[39m\n\u001b[0;32m--> 425\u001b[0m result \u001b[39m=\u001b[39m _cm_type\u001b[39m.\u001b[39;49m\u001b[39m__enter__\u001b[39;49m(cm)\n\u001b[1;32m    426\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_push_cm_exit(cm, _exit)\n\u001b[1;32m    427\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.8/contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/fileio.py:78\u001b[0m, in \u001b[0;36mCache.stream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m@contextlib\u001b[39m\u001b[39m.\u001b[39mcontextmanager\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstream\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverify()\n\u001b[1;32m     79\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path\u001b[39m.\u001b[39mopen(\u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     80\u001b[0m         \u001b[39myield\u001b[39;00m f\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/fileio.py:69\u001b[0m, in \u001b[0;36mCache.verify\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streamer\u001b[39m.\u001b[39mstream() \u001b[39mas\u001b[39;00m stream:\n\u001b[0;32m---> 69\u001b[0m         shutil\u001b[39m.\u001b[39;49mcopyfileobj(stream, f)\n\u001b[1;32m     70\u001b[0m     f\u001b[39m.\u001b[39mclose() \u001b[39m# close file before move... Needed because of Windows\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     shutil\u001b[39m.\u001b[39mmove(f\u001b[39m.\u001b[39mname, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path)\n",
      "File \u001b[0;32m/usr/lib/python3.8/shutil.py:205\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    203\u001b[0m fdst_write \u001b[39m=\u001b[39m fdst\u001b[39m.\u001b[39mwrite\n\u001b[1;32m    204\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     buf \u001b[39m=\u001b[39m fsrc_read(length)\n\u001b[1;32m    206\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m buf:\n\u001b[1;32m    207\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/fileio.py:35\u001b[0m, in \u001b[0;36mIterStream.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mwhile\u001b[39;00m pos \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(b):\n\u001b[1;32m     34\u001b[0m     l \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(b) \u001b[39m-\u001b[39m pos  \u001b[39m# We're supposed to return at most this much\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     chunk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleftover \u001b[39mor\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mit)\n\u001b[1;32m     36\u001b[0m     output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleftover \u001b[39m=\u001b[39m chunk[:l], chunk[l:]\n\u001b[1;32m     37\u001b[0m     b[pos:pos\u001b[39m+\u001b[39m\u001b[39mlen\u001b[39m(output)] \u001b[39m=\u001b[39m output\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/datasets/msmarco_passage.py:81\u001b[0m, in \u001b[0;36mFixEncoding.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m# Find sequences of up to 4 characters that contain a suspicious character.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m# We'll attempt to interpret these as latin1 characters and then decode them back to UTF8.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m# With this technique, we get 100% matches with MS MARCO QnA passages (which do not have this encoding issue)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39m# This approach is more than twice as fast as using ftfy\u001b[39;00m\n\u001b[1;32m     76\u001b[0m regexes \u001b[39m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m     re\u001b[39m.\u001b[39mcompile(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(...\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m|..\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m.|.\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m..|\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m...)\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     78\u001b[0m     re\u001b[39m.\u001b[39mcompile(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(..\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m|.\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m.|\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m..)\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     79\u001b[0m     re\u001b[39m.\u001b[39mcompile(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(.\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m|\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m.)\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     80\u001b[0m ]\n\u001b[0;32m---> 81\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streamer\u001b[39m.\u001b[39mstream() \u001b[39mas\u001b[39;00m stream, \\\n\u001b[1;32m     82\u001b[0m      _logger\u001b[39m.\u001b[39mpbar_raw(desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfixing encoding\u001b[39m\u001b[39m'\u001b[39m, unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m     83\u001b[0m      \u001b[39m# NOTE: codecs.getreader is subtly broken here; it sometimes splits lines between special characters (and it's unclear why)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m stream:\n\u001b[1;32m     85\u001b[0m         pbar\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(line))\n",
      "File \u001b[0;32m/usr/lib/python3.8/contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/fileio.py:96\u001b[0m, in \u001b[0;36mTarExtract.stream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m@contextlib\u001b[39m\u001b[39m.\u001b[39mcontextmanager\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstream\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mExitStack() \u001b[39mas\u001b[39;00m ctxt, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streamer\u001b[39m.\u001b[39mstream() \u001b[39mas\u001b[39;00m stream:\n\u001b[1;32m     97\u001b[0m         \u001b[39m# IMPORTANT: open this file in streaming mode (| in mode). This means that the\u001b[39;00m\n\u001b[1;32m     98\u001b[0m         \u001b[39m# content need not be written to disk or be fully read.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m         tarf \u001b[39m=\u001b[39m ctxt\u001b[39m.\u001b[39menter_context(tarfile\u001b[39m.\u001b[39mopen(fileobj\u001b[39m=\u001b[39mstream, mode\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mr|\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compression\u001b[39m \u001b[39m\u001b[39mor\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m))\n\u001b[1;32m    100\u001b[0m         \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m tarf:\n",
      "File \u001b[0;32m/usr/lib/python3.8/contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/download.py:275\u001b[0m, in \u001b[0;36mDownload.stream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[39myield\u001b[39;00m stream\n\u001b[1;32m    274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath(), \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    276\u001b[0m         \u001b[39myield\u001b[39;00m f\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/download.py:252\u001b[0m, in \u001b[0;36mDownload.path\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[39mwith\u001b[39;00m mirror\u001b[39m.\u001b[39mstream() \u001b[39mas\u001b[39;00m stream:\n\u001b[1;32m    251\u001b[0m             stream \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mHashStream(stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpected_md5, algo\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmd5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 252\u001b[0m             shutil\u001b[39m.\u001b[39;49mcopyfileobj(stream, f)\n\u001b[1;32m    253\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.8/shutil.py:205\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    203\u001b[0m fdst_write \u001b[39m=\u001b[39m fdst\u001b[39m.\u001b[39mwrite\n\u001b[1;32m    204\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     buf \u001b[39m=\u001b[39m fsrc_read(length)\n\u001b[1;32m    206\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m buf:\n\u001b[1;32m    207\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/hash.py:48\u001b[0m, in \u001b[0;36mHashStream.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadinto\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[0;32m---> 48\u001b[0m     count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m     49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verifier\u001b[39m.\u001b[39mupdate(b[:count])\n\u001b[1;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m count \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/fileio.py:35\u001b[0m, in \u001b[0;36mIterStream.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mwhile\u001b[39;00m pos \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(b):\n\u001b[1;32m     34\u001b[0m     l \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(b) \u001b[39m-\u001b[39m pos  \u001b[39m# We're supposed to return at most this much\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     chunk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleftover \u001b[39mor\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mit)\n\u001b[1;32m     36\u001b[0m     output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleftover \u001b[39m=\u001b[39m chunk[:l], chunk[l:]\n\u001b[1;32m     37\u001b[0m     b[pos:pos\u001b[39m+\u001b[39m\u001b[39mlen\u001b[39m(output)] \u001b[39m=\u001b[39m output\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/download.py:97\u001b[0m, in \u001b[0;36mRequestsDownload.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m         pbar_f \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m# defaults to stderr\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     pbar \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(_logger\u001b[39m.\u001b[39mpbar_raw(desc\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl, total\u001b[39m=\u001b[39mdlen, unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, bar_format\u001b[39m=\u001b[39mfmt, file\u001b[39m=\u001b[39mpbar_f))\n\u001b[0;32m---> 97\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_response_data(response, http_args, skip):\n\u001b[1;32m     98\u001b[0m     pbar\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(data))\n\u001b[1;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39maccept-ranges\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbytes\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    100\u001b[0m         \u001b[39m# since we got more data and the server accepts range requests, reset the \"tries\" counter\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/download.py:145\u001b[0m, in \u001b[0;36mRequestsDownload._iter_response_data\u001b[0;34m(self, response, http_args, skip)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     data_iter \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39mio\u001b[39m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[0;32m--> 145\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m data_iter:\n\u001b[1;32m    146\u001b[0m     \u001b[39mif\u001b[39;00m skip \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    147\u001b[0m         data, skipped \u001b[39m=\u001b[39m data[skip:], \u001b[39mlen\u001b[39m(data[:skip])\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/urllib3/response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 935\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    937\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    938\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/urllib3/response.py:874\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[1;32m    872\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[0;32m--> 874\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[1;32m    876\u001b[0m flush_decoder \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/urllib3/response.py:809\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    806\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    808\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 809\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    810\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    811\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    812\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    818\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    819\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/urllib3/response.py:794\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    792\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    793\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 459\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:503\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    498\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[1;32m    500\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    505\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# dataset = pt.get_dataset('irds:msmarco-passage/dev/small')\n",
    "# # index_ref = pt.IndexRef.of('./indices/msmarco-passage') # assumes you have already built an index\n",
    "# indexer = pt.index.IterDictIndexer('./msmarco-passage')\n",
    "# indexref = indexer.index(dataset.get_corpus_iter())\n",
    "# index = pt.IndexFactory.of(indexref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = {\n",
    "    'q1': {\n",
    "        'doc1': {'rank': 1, 'score': 0.5},\n",
    "        'doc2': {'rank': 2, 'score': 0.4},\n",
    "        'doc3': {'rank': 3, 'score': 0.3},\n",
    "        'doc4': {'rank': 4, 'score': 0.2},\n",
    "        'doc5': {'rank': 5, 'score': 0.1},\n",
    "    },\n",
    "    'q2': {\n",
    "        'doc6': {'rank': 1, 'score': 0.6},\n",
    "        'doc7': {'rank': 2, 'score': 0.5},\n",
    "        'doc8': {'rank': 3, 'score': 0.4},\n",
    "        'doc9': {'rank': 4, 'score': 0.3},\n",
    "        'doc10': {'rank': 5, 'score': 0.2},\n",
    "    },\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Create a dictionary of query ids and their corresponding relevance judgements\n",
    "qrels = {\n",
    "    'q1': {\n",
    "        'doc1': 1,\n",
    "        'doc2': 0,\n",
    "        'doc3': 1,\n",
    "        'doc4': 0,\n",
    "        'doc5': 1,\n",
    "    },\n",
    "    'q2': {\n",
    "        'doc6': 0,\n",
    "        'doc7': 1,\n",
    "        'doc8': 1,\n",
    "        'doc9': 0,\n",
    "        'doc10': 0,\n",
    "    },\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Create a dictionary of query ids and their corresponding information need descriptions\n",
    "queries = {\n",
    "    'q1': 'What is the capital of France?',\n",
    "    'q2': 'Who is the author of The Catcher in the Rye?',\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Create a dictionary of document ids and their corresponding text contents\n",
    "documents = {\n",
    "    'doc1': 'Paris is the capital of France.',\n",
    "    'doc2': 'France is a country in Europe.',\n",
    "    'doc3': 'The Eiffel Tower is located in Paris.',\n",
    "    'doc4': 'Germany is a country in Europe.',\n",
    "    'doc5': 'The Louvre is a museum located in Paris.',\n",
    "    'doc6': 'The Catcher in the Rye is a novel by J.D. Salinger.',\n",
    "    'doc7': 'J.D. Salinger was an American writer.',\n",
    "    'doc8': 'The Catcher in the Rye is a coming-of-age novel.',\n",
    "    'doc9': 'To Kill a Mockingbird is a novel by Harper Lee.',\n",
    "    'doc10': 'Harper Lee was an American novelist.',\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Create a dictionary of document ids and their corresponding relevance levels for each query\n",
    "relevance_levels = {\n",
    "    'q1': {\n",
    "        'doc1': 1,\n",
    "        'doc2': 0,\n",
    "        'doc3': 2,\n",
    "        'doc4': 0,\n",
    "        'doc5': 1,\n",
    "    },\n",
    "    'q2': {\n",
    "        'doc6': 2,\n",
    "        'doc7': 2,\n",
    "        'doc8': 1,\n",
    "        'doc9': 0,\n",
    "        'doc10': 0,\n",
    "    },\n",
    "    # ...\n",
    "}\n",
    "\n",
    "documents = [\n",
    "    {'id': 'doc1', 'contents': 'Paris is the capital of France.'},\n",
    "    {'id': 'doc2', 'contents': 'France is a country in Europe.'},\n",
    "    {'id': 'doc3', 'contents': 'The Eiffel Tower is located in Paris.'},\n",
    "    {'id': 'doc4', 'contents': 'Germany is a country in Europe.'},\n",
    "    {'id': 'doc5', 'contents': 'The Louvre is a museum located in Paris.'},\n",
    "    {'id': 'doc6', 'contents': 'The Catcher in the Rye is a novel by J.D. Salinger.'},\n",
    "    {'id': 'doc7', 'contents': 'J.D. Salinger was an American writer.'},\n",
    "    {'id': 'doc8', 'contents': 'The Catcher in the Rye is a coming-of-age novel.'},\n",
    "    {'id': 'doc9', 'contents': 'To Kill a Mockingbird is a novel by Harper Lee.'},\n",
    "    {'id': 'doc10', 'contents': 'Harper Lee was an American novelist.'},\n",
    "    # ...\n",
    "]\n",
    "\n",
    "with open('json/output.json', 'w') as f:\n",
    "    for doc in documents:\n",
    "        f.write(json.dumps(doc) + '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = './indeks/'\n",
    "\n",
    "!python -m pyserini.index.lucene \\\n",
    "    --collection JsonCollection \\\n",
    "    --input json/ \\\n",
    "    --index ./indeks/ \\\n",
    "    --generator DefaultLuceneDocumentGenerator \\\n",
    "    --threads 1 \\\n",
    "    --storeContents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "from pyserini.search import LuceneSearcher\n",
    "import pytrec_eval\n",
    "index_path = './indeks/'\n",
    "# Load the index using a LuceneSearcher object\n",
    "\n",
    "searcher = LuceneSearcher(index_path)\n",
    "\n",
    "#Set the number of results to retrieve per query\n",
    "hits_per_query = 10\n",
    "searcher.set_bm25(0.9, 0.4)\n",
    "\n",
    "# Define the queries, relevance judgments, and information need descriptions\n",
    "queries = {\n",
    "    'q1': 'What is the capital of France?',\n",
    "    'q2': 'Who is the author of The Catcher in the Rye?',\n",
    "    # ...\n",
    "}\n",
    "\n",
    "qrels = {\n",
    "    'q1': {\n",
    "        'doc1': 1,\n",
    "        'doc2': 0,\n",
    "        'doc3': 1,\n",
    "        'doc4': 0,\n",
    "        'doc5': 1,\n",
    "    },\n",
    "    'q2': {\n",
    "        'doc6': 0,\n",
    "        'doc7': 1,\n",
    "        'doc8': 1,\n",
    "        'doc9': 0,\n",
    "        'doc10': 0,\n",
    "    },\n",
    "    # ...\n",
    "}\n",
    "\n",
    "information_need_descriptions = {\n",
    "    'q1': 'Find information about the capital city of France.',\n",
    "    'q2': 'Find information about the author of The Catcher in the Rye.',\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Define the evaluation measures to compute\n",
    "measures = {'recip_rank', 'map', 'ndcg'}\n",
    "\n",
    "# Compute the query results and convert them to the format required by Pytrec_eval\n",
    "results = {}\n",
    "for query_id, query_text in queries.items():\n",
    "    hits = searcher.search(query_text, k=hits_per_query)\n",
    "    query_results = {hit.docid: {'rank': i+1, 'score': hit.score} for i, hit in enumerate(hits)}\n",
    "    results[query_id] = query_results\n",
    "print(results)\n",
    "pytrec_eval_qrels = (qrels)\n",
    "\n",
    "# Convert the results dictionary into the format required by Pytrec_eval\n",
    "pytrec_eval_results = {}\n",
    "for query_id, docs in results.items():\n",
    "    for rank, (doc_id, doc_info) in enumerate(docs.items()):\n",
    "        score = doc_info['score']\n",
    "        pytrec_eval_results.setdefault(query_id, {})[doc_id] = score\n",
    "\n",
    "\n",
    "# Create a Pytrec_eval evaluator object and compute the evaluation scores\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(pytrec_eval_qrels, measures)\n",
    "evaluation_scores = evaluator.evaluate(pytrec_eval_results)\n",
    "\n",
    "# Print the evaluation scores for each query\n",
    "for query_id, query_text in queries.items():\n",
    "    print(f\"Evaluation results for query '{query_text}':\")\n",
    "    print(json.dumps(evaluation_scores[query_id], indent=2))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pytrec_eval evaluator object and compute the evaluation scores\n",
    "\n",
    "# Get the MRR score from the Pytrec_eval results\n",
    "mrr = pytrec_eval_results['recip_rank']['all']['mean_reciprocal_rank']\n",
    "pytrec_eval_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyserini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
