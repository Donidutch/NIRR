{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "import json\n",
    "import pandas as pd\n",
    "# pt.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index Variant</th>\n",
       "      <th>Ranking Model</th>\n",
       "      <th>NDCG</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <th>NDCG@20</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Precision@20</th>\n",
       "      <th>Recall@5</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>Recall@20</th>\n",
       "      <th>MRR</th>\n",
       "      <th>Mean Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stopwords_removed_stemming</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stopwords_removed_stemming</td>\n",
       "      <td>LM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Index Variant Ranking Model  NDCG  NDCG@5  NDCG@10  NDCG@20   \n",
       "0  stopwords_removed_stemming          BM25   0.0     0.0      0.0      0.0  \\\n",
       "1  stopwords_removed_stemming            LM   0.0     0.0      0.0      0.0   \n",
       "\n",
       "   Precision@5  Precision@10  Precision@20  Recall@5  Recall@10  Recall@20   \n",
       "0          0.0           0.0           0.0       0.0        0.0        0.0  \\\n",
       "1          0.0           0.0           0.0       0.0        0.0        0.0   \n",
       "\n",
       "   MRR  Mean Time  \n",
       "0  0.0   0.000106  \n",
       "1  0.0   0.000104  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('output/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Please confirm you agree to the MSMARCO data usage agreement found at <http://www.msmarco.org/dataset.aspx>\n",
      "[INFO] If you have a local copy of https://msmarco.blob.core.windows.net/msmarcoranking/collectionandqueries.tar.gz, you can symlink it here to avoid downloading it again: /home/nicklas/.ir_datasets/downloads/31644046b18952c1386cd4564ba2ae69\n",
      "[INFO] [starting] https://msmarco.blob.core.windows.net/msmarcoranking/collectionandqueries.tar.gz\n",
      "                                                                                \n",
      "\u001b[A                                                                                                                        [INFO] [error] https://msmarco.blob.core.windows.net/msmarcoranking/collectionandqueries.tar.gz: [00:26] [28.2MB] [1.08MB/s]\n",
      "msmarco-passage/dev/small documents:   0%|          | 0/8841823 [00:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m# index_ref = pt.IndexRef.of('./indices/msmarco-passage') # assumes you have already built an index\u001b[39;00m\n\u001b[1;32m      3\u001b[0m indexer \u001b[39m=\u001b[39m pt\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mIterDictIndexer(\u001b[39m'\u001b[39m\u001b[39m./msmarco-passage\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m indexref \u001b[39m=\u001b[39m indexer\u001b[39m.\u001b[39;49mindex(dataset\u001b[39m.\u001b[39;49mget_corpus_iter())\n\u001b[1;32m      5\u001b[0m index \u001b[39m=\u001b[39m pt\u001b[39m.\u001b[39mIndexFactory\u001b[39m.\u001b[39mof(indexref)\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/pyterrier/index.py:968\u001b[0m, in \u001b[0;36m_IterDictIndexer_fifo.index\u001b[0;34m(self, it, fields, meta, meta_lengths)\u001b[0m\n\u001b[1;32m    965\u001b[0m     fifos\u001b[39m.\u001b[39mappend(fifo)\n\u001b[1;32m    967\u001b[0m \u001b[39m# Start dishing out the docs to the fifos\u001b[39;00m\n\u001b[0;32m--> 968\u001b[0m threading\u001b[39m.\u001b[39mThread(target\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_write_fifos, args\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_filter_iterable(it, fields), fifos), daemon\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mstart()\n\u001b[1;32m    970\u001b[0m \u001b[39m# Different process for memory indexer (still taking advantage of faster fifos)\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[39mif\u001b[39;00m Indexer \u001b[39mis\u001b[39;00m BasicMemoryIndexer:\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/pyterrier/index.py:824\u001b[0m, in \u001b[0;36m_BaseIterDictIndexer._filter_iterable\u001b[0;34m(self, it, indexed_fields)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    822\u001b[0m     all_fields \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdocno\u001b[39m\u001b[39m'\u001b[39m} \u001b[39m|\u001b[39m \u001b[39mset\u001b[39m(indexed_fields) \u001b[39m|\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m--> 824\u001b[0m (first_doc,), it \u001b[39m=\u001b[39m more_itertools\u001b[39m.\u001b[39;49mspy(it) \u001b[39m# peek at the first document and validate it\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_doc_dict(first_doc)\n\u001b[1;32m    827\u001b[0m \u001b[39m# important: return an iterator here, rather than make this function a generator,\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[39m# to be sure that the validation above happens when _filter_iterable is called,\u001b[39;00m\n\u001b[1;32m    829\u001b[0m \u001b[39m# rather than on the first invocation of next()\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/more_itertools/more.py:1045\u001b[0m, in \u001b[0;36mspy\u001b[0;34m(iterable, n)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a 2-tuple with a list containing the first *n* elements of\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[39m*iterable*, and an iterator with the same items as *iterable*.\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[39mThis allows you to \"look ahead\" at the items in the iterable without\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \n\u001b[1;32m   1043\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m it \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(iterable)\n\u001b[0;32m-> 1045\u001b[0m head \u001b[39m=\u001b[39m take(n, it)\n\u001b[1;32m   1047\u001b[0m \u001b[39mreturn\u001b[39;00m head\u001b[39m.\u001b[39mcopy(), chain(head, it)\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/more_itertools/recipes.py:93\u001b[0m, in \u001b[0;36mtake\u001b[0;34m(n, iterable)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtake\u001b[39m(n, iterable):\n\u001b[1;32m     81\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return first *n* items of the iterable as a list.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m \u001b[39m        >>> take(3, range(10))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \n\u001b[1;32m     92\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(islice(iterable, n))\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/pyterrier/datasets.py:418\u001b[0m, in \u001b[0;36mIRDSDataset.get_corpus_iter.<locals>.gen\u001b[0;34m()\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgen\u001b[39m():\n\u001b[0;32m--> 418\u001b[0m     \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    419\u001b[0m         doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39m_asdict()\n\u001b[1;32m    420\u001b[0m         \u001b[39m# pyterrier uses \"docno\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/__init__.py:147\u001b[0m, in \u001b[0;36mDocstoreSplitter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mit)\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/formats/tsv.py:92\u001b[0m, in \u001b[0;36mTsvIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 92\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mline_iter)\n\u001b[1;32m     93\u001b[0m     cols \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mrstrip(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m     num_cols \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls\u001b[39m.\u001b[39m_fields)\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/formats/tsv.py:28\u001b[0m, in \u001b[0;36mFileLineIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctxt\u001b[39m.\u001b[39menter_context(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdlc[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_idx]\u001b[39m.\u001b[39mstream()))\n\u001b[1;32m     27\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctxt\u001b[39m.\u001b[39;49menter_context(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdlc\u001b[39m.\u001b[39;49mstream()))\n\u001b[1;32m     29\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart:\n\u001b[1;32m     30\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mreadline()\n",
      "File \u001b[0;32m/usr/lib/python3.8/contextlib.py:425\u001b[0m, in \u001b[0;36m_BaseExitStack.enter_context\u001b[0;34m(self, cm)\u001b[0m\n\u001b[1;32m    423\u001b[0m _cm_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(cm)\n\u001b[1;32m    424\u001b[0m _exit \u001b[39m=\u001b[39m _cm_type\u001b[39m.\u001b[39m\u001b[39m__exit__\u001b[39m\n\u001b[0;32m--> 425\u001b[0m result \u001b[39m=\u001b[39m _cm_type\u001b[39m.\u001b[39;49m\u001b[39m__enter__\u001b[39;49m(cm)\n\u001b[1;32m    426\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_push_cm_exit(cm, _exit)\n\u001b[1;32m    427\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.8/contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/fileio.py:78\u001b[0m, in \u001b[0;36mCache.stream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m@contextlib\u001b[39m\u001b[39m.\u001b[39mcontextmanager\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstream\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverify()\n\u001b[1;32m     79\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path\u001b[39m.\u001b[39mopen(\u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     80\u001b[0m         \u001b[39myield\u001b[39;00m f\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/fileio.py:69\u001b[0m, in \u001b[0;36mCache.verify\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streamer\u001b[39m.\u001b[39mstream() \u001b[39mas\u001b[39;00m stream:\n\u001b[0;32m---> 69\u001b[0m         shutil\u001b[39m.\u001b[39;49mcopyfileobj(stream, f)\n\u001b[1;32m     70\u001b[0m     f\u001b[39m.\u001b[39mclose() \u001b[39m# close file before move... Needed because of Windows\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     shutil\u001b[39m.\u001b[39mmove(f\u001b[39m.\u001b[39mname, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path)\n",
      "File \u001b[0;32m/usr/lib/python3.8/shutil.py:205\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    203\u001b[0m fdst_write \u001b[39m=\u001b[39m fdst\u001b[39m.\u001b[39mwrite\n\u001b[1;32m    204\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     buf \u001b[39m=\u001b[39m fsrc_read(length)\n\u001b[1;32m    206\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m buf:\n\u001b[1;32m    207\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/fileio.py:35\u001b[0m, in \u001b[0;36mIterStream.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mwhile\u001b[39;00m pos \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(b):\n\u001b[1;32m     34\u001b[0m     l \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(b) \u001b[39m-\u001b[39m pos  \u001b[39m# We're supposed to return at most this much\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     chunk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleftover \u001b[39mor\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mit)\n\u001b[1;32m     36\u001b[0m     output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleftover \u001b[39m=\u001b[39m chunk[:l], chunk[l:]\n\u001b[1;32m     37\u001b[0m     b[pos:pos\u001b[39m+\u001b[39m\u001b[39mlen\u001b[39m(output)] \u001b[39m=\u001b[39m output\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/datasets/msmarco_passage.py:81\u001b[0m, in \u001b[0;36mFixEncoding.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m# Find sequences of up to 4 characters that contain a suspicious character.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m# We'll attempt to interpret these as latin1 characters and then decode them back to UTF8.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m# With this technique, we get 100% matches with MS MARCO QnA passages (which do not have this encoding issue)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39m# This approach is more than twice as fast as using ftfy\u001b[39;00m\n\u001b[1;32m     76\u001b[0m regexes \u001b[39m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m     re\u001b[39m.\u001b[39mcompile(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(...\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m|..\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m.|.\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m..|\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m...)\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     78\u001b[0m     re\u001b[39m.\u001b[39mcompile(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(..\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m|.\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m.|\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m..)\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     79\u001b[0m     re\u001b[39m.\u001b[39mcompile(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(.\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m|\u001b[39m\u001b[39m{\u001b[39;00mSUS\u001b[39m}\u001b[39;00m\u001b[39m.)\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     80\u001b[0m ]\n\u001b[0;32m---> 81\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streamer\u001b[39m.\u001b[39mstream() \u001b[39mas\u001b[39;00m stream, \\\n\u001b[1;32m     82\u001b[0m      _logger\u001b[39m.\u001b[39mpbar_raw(desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfixing encoding\u001b[39m\u001b[39m'\u001b[39m, unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m     83\u001b[0m      \u001b[39m# NOTE: codecs.getreader is subtly broken here; it sometimes splits lines between special characters (and it's unclear why)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m stream:\n\u001b[1;32m     85\u001b[0m         pbar\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(line))\n",
      "File \u001b[0;32m/usr/lib/python3.8/contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/fileio.py:96\u001b[0m, in \u001b[0;36mTarExtract.stream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m@contextlib\u001b[39m\u001b[39m.\u001b[39mcontextmanager\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstream\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mExitStack() \u001b[39mas\u001b[39;00m ctxt, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streamer\u001b[39m.\u001b[39mstream() \u001b[39mas\u001b[39;00m stream:\n\u001b[1;32m     97\u001b[0m         \u001b[39m# IMPORTANT: open this file in streaming mode (| in mode). This means that the\u001b[39;00m\n\u001b[1;32m     98\u001b[0m         \u001b[39m# content need not be written to disk or be fully read.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m         tarf \u001b[39m=\u001b[39m ctxt\u001b[39m.\u001b[39menter_context(tarfile\u001b[39m.\u001b[39mopen(fileobj\u001b[39m=\u001b[39mstream, mode\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mr|\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compression\u001b[39m \u001b[39m\u001b[39mor\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m))\n\u001b[1;32m    100\u001b[0m         \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m tarf:\n",
      "File \u001b[0;32m/usr/lib/python3.8/contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/download.py:275\u001b[0m, in \u001b[0;36mDownload.stream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[39myield\u001b[39;00m stream\n\u001b[1;32m    274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath(), \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    276\u001b[0m         \u001b[39myield\u001b[39;00m f\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/download.py:252\u001b[0m, in \u001b[0;36mDownload.path\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[39mwith\u001b[39;00m mirror\u001b[39m.\u001b[39mstream() \u001b[39mas\u001b[39;00m stream:\n\u001b[1;32m    251\u001b[0m             stream \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mHashStream(stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpected_md5, algo\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmd5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 252\u001b[0m             shutil\u001b[39m.\u001b[39;49mcopyfileobj(stream, f)\n\u001b[1;32m    253\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.8/shutil.py:205\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    203\u001b[0m fdst_write \u001b[39m=\u001b[39m fdst\u001b[39m.\u001b[39mwrite\n\u001b[1;32m    204\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     buf \u001b[39m=\u001b[39m fsrc_read(length)\n\u001b[1;32m    206\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m buf:\n\u001b[1;32m    207\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/hash.py:48\u001b[0m, in \u001b[0;36mHashStream.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadinto\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[0;32m---> 48\u001b[0m     count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m     49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verifier\u001b[39m.\u001b[39mupdate(b[:count])\n\u001b[1;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m count \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/fileio.py:35\u001b[0m, in \u001b[0;36mIterStream.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mwhile\u001b[39;00m pos \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(b):\n\u001b[1;32m     34\u001b[0m     l \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(b) \u001b[39m-\u001b[39m pos  \u001b[39m# We're supposed to return at most this much\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     chunk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleftover \u001b[39mor\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mit)\n\u001b[1;32m     36\u001b[0m     output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleftover \u001b[39m=\u001b[39m chunk[:l], chunk[l:]\n\u001b[1;32m     37\u001b[0m     b[pos:pos\u001b[39m+\u001b[39m\u001b[39mlen\u001b[39m(output)] \u001b[39m=\u001b[39m output\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/download.py:97\u001b[0m, in \u001b[0;36mRequestsDownload.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m         pbar_f \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m# defaults to stderr\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     pbar \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(_logger\u001b[39m.\u001b[39mpbar_raw(desc\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl, total\u001b[39m=\u001b[39mdlen, unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, bar_format\u001b[39m=\u001b[39mfmt, file\u001b[39m=\u001b[39mpbar_f))\n\u001b[0;32m---> 97\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_response_data(response, http_args, skip):\n\u001b[1;32m     98\u001b[0m     pbar\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(data))\n\u001b[1;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39maccept-ranges\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbytes\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    100\u001b[0m         \u001b[39m# since we got more data and the server accepts range requests, reset the \"tries\" counter\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/ir_datasets/util/download.py:145\u001b[0m, in \u001b[0;36mRequestsDownload._iter_response_data\u001b[0;34m(self, response, http_args, skip)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     data_iter \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39mio\u001b[39m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[0;32m--> 145\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m data_iter:\n\u001b[1;32m    146\u001b[0m     \u001b[39mif\u001b[39;00m skip \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    147\u001b[0m         data, skipped \u001b[39m=\u001b[39m data[skip:], \u001b[39mlen\u001b[39m(data[:skip])\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/urllib3/response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 935\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    937\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    938\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/urllib3/response.py:874\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[1;32m    872\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[0;32m--> 874\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[1;32m    876\u001b[0m flush_decoder \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/urllib3/response.py:809\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    806\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    808\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 809\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    810\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    811\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    812\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    818\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    819\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desktop/NIR/.venv/lib/python3.8/site-packages/urllib3/response.py:794\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    792\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    793\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 459\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:503\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    498\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[1;32m    500\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    505\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# dataset = pt.get_dataset('irds:msmarco-passage/dev/small')\n",
    "# # index_ref = pt.IndexRef.of('./indices/msmarco-passage') # assumes you have already built an index\n",
    "# indexer = pt.index.IterDictIndexer('./msmarco-passage')\n",
    "# indexref = indexer.index(dataset.get_corpus_iter())\n",
    "# index = pt.IndexFactory.of(indexref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = {\n",
    "    'q1': {\n",
    "        'doc1': {'rank': 1, 'score': 0.5},\n",
    "        'doc2': {'rank': 2, 'score': 0.4},\n",
    "        'doc3': {'rank': 3, 'score': 0.3},\n",
    "        'doc4': {'rank': 4, 'score': 0.2},\n",
    "        'doc5': {'rank': 5, 'score': 0.1},\n",
    "    },\n",
    "    'q2': {\n",
    "        'doc6': {'rank': 1, 'score': 0.6},\n",
    "        'doc7': {'rank': 2, 'score': 0.5},\n",
    "        'doc8': {'rank': 3, 'score': 0.4},\n",
    "        'doc9': {'rank': 4, 'score': 0.3},\n",
    "        'doc10': {'rank': 5, 'score': 0.2},\n",
    "    },\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Create a dictionary of query ids and their corresponding relevance judgements\n",
    "qrels = {\n",
    "    'q1': {\n",
    "        'doc1': 1,\n",
    "        'doc2': 0,\n",
    "        'doc3': 1,\n",
    "        'doc4': 0,\n",
    "        'doc5': 1,\n",
    "    },\n",
    "    'q2': {\n",
    "        'doc6': 0,\n",
    "        'doc7': 1,\n",
    "        'doc8': 1,\n",
    "        'doc9': 0,\n",
    "        'doc10': 0,\n",
    "    },\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Create a dictionary of query ids and their corresponding information need descriptions\n",
    "queries = {\n",
    "    'q1': 'What is the capital of France?',\n",
    "    'q2': 'Who is the author of The Catcher in the Rye?',\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Create a dictionary of document ids and their corresponding text contents\n",
    "documents = {\n",
    "    'doc1': 'Paris is the capital of France.',\n",
    "    'doc2': 'France is a country in Europe.',\n",
    "    'doc3': 'The Eiffel Tower is located in Paris.',\n",
    "    'doc4': 'Germany is a country in Europe.',\n",
    "    'doc5': 'The Louvre is a museum located in Paris.',\n",
    "    'doc6': 'The Catcher in the Rye is a novel by J.D. Salinger.',\n",
    "    'doc7': 'J.D. Salinger was an American writer.',\n",
    "    'doc8': 'The Catcher in the Rye is a coming-of-age novel.',\n",
    "    'doc9': 'To Kill a Mockingbird is a novel by Harper Lee.',\n",
    "    'doc10': 'Harper Lee was an American novelist.',\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Create a dictionary of document ids and their corresponding relevance levels for each query\n",
    "relevance_levels = {\n",
    "    'q1': {\n",
    "        'doc1': 1,\n",
    "        'doc2': 0,\n",
    "        'doc3': 2,\n",
    "        'doc4': 0,\n",
    "        'doc5': 1,\n",
    "    },\n",
    "    'q2': {\n",
    "        'doc6': 2,\n",
    "        'doc7': 2,\n",
    "        'doc8': 1,\n",
    "        'doc9': 0,\n",
    "        'doc10': 0,\n",
    "    },\n",
    "    # ...\n",
    "}\n",
    "\n",
    "documents = [\n",
    "    {'id': 'doc1', 'contents': 'Paris is the capital of France.'},\n",
    "    {'id': 'doc2', 'contents': 'France is a country in Europe.'},\n",
    "    {'id': 'doc3', 'contents': 'The Eiffel Tower is located in Paris.'},\n",
    "    {'id': 'doc4', 'contents': 'Germany is a country in Europe.'},\n",
    "    {'id': 'doc5', 'contents': 'The Louvre is a museum located in Paris.'},\n",
    "    {'id': 'doc6', 'contents': 'The Catcher in the Rye is a novel by J.D. Salinger.'},\n",
    "    {'id': 'doc7', 'contents': 'J.D. Salinger was an American writer.'},\n",
    "    {'id': 'doc8', 'contents': 'The Catcher in the Rye is a coming-of-age novel.'},\n",
    "    {'id': 'doc9', 'contents': 'To Kill a Mockingbird is a novel by Harper Lee.'},\n",
    "    {'id': 'doc10', 'contents': 'Harper Lee was an American novelist.'},\n",
    "    # ...\n",
    "]\n",
    "\n",
    "with open('json/output.json', 'w') as f:\n",
    "    for doc in documents:\n",
    "        f.write(json.dumps(doc) + '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = './indeks/'\n",
    "\n",
    "!python -m pyserini.index.lucene \\\n",
    "    --collection JsonCollection \\\n",
    "    --input json/ \\\n",
    "    --index ./indeks/ \\\n",
    "    --generator DefaultLuceneDocumentGenerator \\\n",
    "    --threads 1 \\\n",
    "    --storeContents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "from pyserini.search import LuceneSearcher\n",
    "import pytrec_eval\n",
    "index_path = './indeks/'\n",
    "# Load the index using a LuceneSearcher object\n",
    "\n",
    "searcher = LuceneSearcher(index_path)\n",
    "\n",
    "#Set the number of results to retrieve per query\n",
    "hits_per_query = 10\n",
    "searcher.set_bm25(0.9, 0.4)\n",
    "\n",
    "# Define the queries, relevance judgments, and information need descriptions\n",
    "queries = {\n",
    "    'q1': 'What is the capital of France?',\n",
    "    'q2': 'Who is the author of The Catcher in the Rye?',\n",
    "    # ...\n",
    "}\n",
    "\n",
    "qrels = {\n",
    "    'q1': {\n",
    "        'doc1': 1,\n",
    "        'doc2': 0,\n",
    "        'doc3': 1,\n",
    "        'doc4': 0,\n",
    "        'doc5': 1,\n",
    "    },\n",
    "    'q2': {\n",
    "        'doc6': 0,\n",
    "        'doc7': 1,\n",
    "        'doc8': 1,\n",
    "        'doc9': 0,\n",
    "        'doc10': 0,\n",
    "    },\n",
    "    # ...\n",
    "}\n",
    "\n",
    "information_need_descriptions = {\n",
    "    'q1': 'Find information about the capital city of France.',\n",
    "    'q2': 'Find information about the author of The Catcher in the Rye.',\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Define the evaluation measures to compute\n",
    "measures = {'recip_rank', 'map', 'ndcg'}\n",
    "\n",
    "# Compute the query results and convert them to the format required by Pytrec_eval\n",
    "results = {}\n",
    "for query_id, query_text in queries.items():\n",
    "    hits = searcher.search(query_text, k=hits_per_query)\n",
    "    query_results = {hit.docid: {'rank': i+1, 'score': hit.score} for i, hit in enumerate(hits)}\n",
    "    results[query_id] = query_results\n",
    "print(results)\n",
    "pytrec_eval_qrels = (qrels)\n",
    "\n",
    "# Convert the results dictionary into the format required by Pytrec_eval\n",
    "pytrec_eval_results = {}\n",
    "for query_id, docs in results.items():\n",
    "    for rank, (doc_id, doc_info) in enumerate(docs.items()):\n",
    "        score = doc_info['score']\n",
    "        pytrec_eval_results.setdefault(query_id, {})[doc_id] = score\n",
    "\n",
    "\n",
    "# Create a Pytrec_eval evaluator object and compute the evaluation scores\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(pytrec_eval_qrels, measures)\n",
    "evaluation_scores = evaluator.evaluate(pytrec_eval_results)\n",
    "\n",
    "# Print the evaluation scores for each query\n",
    "for query_id, query_text in queries.items():\n",
    "    print(f\"Evaluation results for query '{query_text}':\")\n",
    "    print(json.dumps(evaluation_scores[query_id], indent=2))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pytrec_eval evaluator object and compute the evaluation scores\n",
    "\n",
    "# Get the MRR score from the Pytrec_eval results\n",
    "mrr = pytrec_eval_results['recip_rank']['all']['mean_reciprocal_rank']\n",
    "pytrec_eval_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyserini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
